--------------------------------------------------
Nice notes about LadderVAE model

1. The `z_dims` parameter:
    - `z_dims` is a list of int that determines the size of latent tensors (specifically, the # of channels) in the top-down layers.
    As latent tensor I mean the `z` tensors sampled from the inference distribution q(z).
    - It is NOT used in the Encoder, where the # of channels of the latents is set by `n_filters`.
    - Specifically, `z_dims` is important for 2 facts:
        1. The length of the list determines the number of layers (`n_layers`) in both the Encoder and the Decoder.
        2. It is used in the `NormalStochasticBlock2d`, where the inputs `p_params` and `q_params` are mapped to `2*z_dims` # of channels using 1x1 convolutions. 

2. Don't be fooled by the number of ouput channels in the Gated layer! Indeed, this block initially doubles the number of channels through a convolutional layer.
However, it then split the result in 2 chunks and uses one half of the channels as gate (i.e., out = x[first_half] * sigmoid(x[second_half])).
Therefore, the number of channels in the output equals the number of channels in the input.

3. The difference in the behaviour of the top-most TopDownLayer and the other is that:
    - In the topmost, `p_params` are obtained from the prior --> shape: [B, 2*z_dims, img_shape[0]//overall_downsc, img_shape[1]//overall_downsc]
    - `q_params` is simply obtained from the associated `bu_value` tensor, and not through combination with `p_params`.
On the contrary, for subsequent TopDownLayer's, the p_params tensor is obtained from input_ argument, and q_params is obtained by merging p_params with
the bu_value tensor relative to that layer.

4. The `transform_p_params` in `NormalStochasticBlock2d` is always true expecpt made for the top-most `TopDownLayer`.
Indeed, in that case the # of channels of `p_params` is already 2*z_dims, so it doesn't need any further transformation.

5. About _multiscale_count used in LC. Observe that _multiscale_count defines the total number of inputs to the bottom-up pass.
In other terms if we have the input patch and n_LC additional lateral inputs, we will have a total of (n_LC + 1) inputs.
The rationale behind this implementation choice is that Ashesh regards the lateral inputs in the same way as the input patch
to the primary flow of the bottom-up pass. -> POSSIBLE REFACTORING

6. With the default parameter configuration we have `encoder_blocks_per_layer == 1`. Hence, each bottom-up layer only contain one `BottomUpDeterministicResBlock` which
performs downsampling. Therefore, there are no additional blocks to apply further convolutions on both the primary flow and the input lateral tensor (specifically, in 
the code we have `self.net()` and `self.lowres_net()` as empty `nn.Module`'s).

7. The `nn.ModuleList` `top_down_layers` stores the `TopDownLayer` modules. It is important to notice that the layers are stored from the bottom-most (i=0), 
to the top-most (i = n_layers - 1).

--------------------------------------------------
Notes about LightningModule for training the model

1. In `training_step()` method of the Lightning module, most of the code is related to computing the loss for the model. 
It would be nice to store all of this code into a different `compute_loss()` method, which simply receive as input the result of forward method.

2. `reconstruction_mode` is just for experiments. In reality, it is always False, and what changes is what we pass as target to the model. Specifically:
- HDN (unsupervised denoising) will get as target the same input patch (see LadderVAEDenoiser to see how it is done).
- muSplit (supervised unmixing) will get as target the unmixed image instead.
Observe that the current implementation of the DataLoader is common to the two. 
What differentiate the input to the model is, of course, the nature of input data and how these are handled in the model class itself.

3. An implementation note: attributes used only in the Lightning wrapper (e.g., most of loss and training related attributes) are directly defined there, 
so that we don't have to call self.model.some_attribute to used them. Also this make code more compartimentalized and clean.
--> things that would be nice to move:
    - data_mean, data_std --> not used in the model definition...
    - likelihood module & loss_type (they go together) --> Ashesh suggested to keep things like they are

4. Ask Ashesh to help understanding implementation details of patching / tiling