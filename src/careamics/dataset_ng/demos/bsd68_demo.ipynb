{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from careamics_portfolio import PortfolioManager\n",
    "\n",
    "from careamics.config.configuration_factories import create_n2v_configuration\n",
    "from careamics.lightning.dataset_ng.lightning_module import N2VModule\n",
    "from careamics.lightning.dataset_ng.train_data_module import TrainDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data portfolio manage\n",
    "portfolio = PortfolioManager()\n",
    "\n",
    "# and download the data\n",
    "root_path = Path(\"./data\")\n",
    "files = portfolio.denoising.N2V_BSD68.download(root_path)\n",
    "\n",
    "# create paths for the data\n",
    "data_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\")\n",
    "train_path = data_path / \"train\"\n",
    "val_path = data_path / \"val\"\n",
    "test_path = data_path / \"test\" / \"images\"\n",
    "gt_path = data_path / \"test\" / \"gt\"\n",
    "\n",
    "# load training and validation image and show them side by side\n",
    "train_files = sorted(train_path.rglob(\"*.tiff\"))\n",
    "val_files = sorted(val_path.rglob(\"*.tiff\"))\n",
    "test_files = sorted(test_path.rglob(\"*.tiff\"))\n",
    "\n",
    "single_train_image = tifffile.imread(train_files[0])[0]\n",
    "single_val_image = tifffile.imread(val_files[0])[0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(single_train_image, cmap=\"gray\")\n",
    "ax[0].set_title(\"Training Image\")\n",
    "ax[1].imshow(single_val_image, cmap=\"gray\")\n",
    "ax[1].set_title(\"Validation Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_n2v_configuration(\n",
    "    experiment_name=\"bsd68_n2v\",\n",
    "    data_type=\"tiff\",\n",
    "    axes=\"SYX\",\n",
    "    patch_size=(64, 64),\n",
    "    batch_size=64,\n",
    "    num_epochs=100\n",
    ")\n",
    "\n",
    "config.data_config.set_means_and_stds([single_train_image.mean()], [single_train_image.std()] )\n",
    "\n",
    "model = N2VModule(config.algorithm_config)\n",
    "\n",
    "\n",
    "data_module = TrainDataModule(\n",
    "    data_config=config.data_config,\n",
    "    train_data=train_files,\n",
    "    val_data=val_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = next(iter(data_module.train_dataloader()))\n",
    "val_batch = next(iter(data_module.val_dataloader()))\n",
    "\n",
    "fig, ax = plt.subplots(1, 8, figsize=(10, 5))\n",
    "\n",
    "for i in range(8):\n",
    "    ax[i].imshow(train_batch.data[0][i][0].numpy(), cmap=\"gray\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 8, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax[i].imshow(val_batch.data[0][i][0].numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "root = Path(\"bsd68_n2v\")\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=root / \"checkpoints\",\n",
    "        filename=\"bsd68_lightning_api\",\n",
    "        save_last=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create a Lightning Trainer\n",
    "trainer = Trainer(max_epochs=100, default_root_dir=root, callbacks=callbacks)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from careamics.config.inference_model import InferenceConfig\n",
    "from careamics.lightning.dataset_ng.predict_data_module import PredictDataModule\n",
    "\n",
    "config = InferenceConfig(\n",
    "    model_config=config,\n",
    "    data_type=\"tiff\",\n",
    "    tile_size=(128, 128),\n",
    "    tile_overlap=(32, 32),\n",
    "    axes=\"YX\",\n",
    "    batch_size=1,\n",
    "    image_means=data_module.train_dataset.input_stats.means,\n",
    "    image_stds=data_module.train_dataset.input_stats.stds\n",
    ")\n",
    "\n",
    "inference_data_module = PredictDataModule(\n",
    "    data_config=config,\n",
    "    pred_data=test_files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, datamodule=inference_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data_samples(prediction):\n",
    "    result = []\n",
    "\n",
    "    items = []\n",
    "    current_sample_id = 0\n",
    "    for pred in prediction:\n",
    "        if pred.region_spec['data_idx'] == current_sample_id:\n",
    "            items.append(pred)\n",
    "        else:\n",
    "            result.append(items)\n",
    "            items = []\n",
    "            current_sample_id = pred.region_spec['data_idx']\n",
    "            items.append(pred)\n",
    "    return result\n",
    "\n",
    "\n",
    "samples = gather_data_samples(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for sample in samples:\n",
    "    input_shape = sample[0].data_shape\n",
    "    predicted_image = np.zeros(input_shape, dtype=np.float32)\n",
    "    for pred in sample:\n",
    "        tile_array = pred.data\n",
    "        tile_info = pred.region_spec\n",
    "        data_idx = tile_info['data_idx']\n",
    "        crop_coords = tile_info['crop_coords']\n",
    "        crop_sizes = tile_info['crop_size']\n",
    "        stitch_coords = tile_info['stitch_coords']\n",
    "\n",
    "        crop_slices = []\n",
    "        for coord, size in zip(crop_coords, crop_sizes):\n",
    "            crop_slice = slice(coord, coord + size)\n",
    "            crop_slices.append(crop_slice)\n",
    "\n",
    "        crop_slices = (\n",
    "            ...,\n",
    "            *crop_slices,\n",
    "        )\n",
    "\n",
    "        cropped_tile = tile_array[crop_slices]\n",
    "\n",
    "        stitch_slices = []\n",
    "        for coord, size in zip(stitch_coords, crop_sizes):\n",
    "            stitch_slice = slice(coord, coord + size)\n",
    "            stitch_slices.append(stitch_slice)\n",
    "\n",
    "        stitch_slices = (..., *stitch_slices)\n",
    "\n",
    "        predicted_image[stitch_slices] = cropped_tile.astype(np.float32)\n",
    "\n",
    "    prediction.append(predicted_image[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from careamics.utils.metrics import psnr, scale_invariant_psnr\n",
    "\n",
    "# Show two images\n",
    "noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\n",
    "gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n",
    "\n",
    "# images to show\n",
    "images = np.random.choice(range(len(noises)), 3)\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(3):\n",
    "    pred_image = prediction[images[i]].squeeze()\n",
    "    psnr_noisy = psnr(gts[images[i]], noises[images[i]], data_range=gts[images[i]].max() - gts[images[i]].min())\n",
    "    psnr_result = psnr(gts[images[i]], pred_image, data_range=gts[images[i]].max() - gts[images[i]].min())\n",
    "\n",
    "    scale_invariant_psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)\n",
    "\n",
    "    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n",
    "    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n",
    "\n",
    "    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n",
    "    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\\nScale invariant PSNR: {scale_invariant_psnr_result:.2f}\")\n",
    "\n",
    "    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n",
    "    ax[i, 2].title.set_text(\"Ground-truth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnrs = np.zeros((len(prediction), 1))\n",
    "scale_invariant_psnrs = np.zeros((len(prediction), 1))\n",
    "\n",
    "for i, (pred, gt) in enumerate(zip(prediction, gts)):\n",
    "    psnrs[i] = psnr(gt, pred.squeeze(), data_range=gt.max() - gt.min())\n",
    "    scale_invariant_psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n",
    "\n",
    "print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\n",
    "print(f\"Scale invariant PSNR: {scale_invariant_psnrs.mean():.2f} +/- {scale_invariant_psnrs.std():.2f}\")\n",
    "print(\"Reported PSNR: 27.71\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
