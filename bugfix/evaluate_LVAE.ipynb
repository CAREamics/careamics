{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Literal, Optional, Union\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from careamics.config.likelihood_model import NMLikelihoodConfig\n",
    "from careamics.config.nm_model import MultiChannelNMConfig\n",
    "from careamics.lightning import VAEModule\n",
    "from careamics.lvae_training.data_modules import LCMultiChDloader, MultiChDloader\n",
    "from careamics.lvae_training.data_utils import (\n",
    "    DataSplitType,\n",
    "    DataType,\n",
    "    GridAlignement,\n",
    "    load_tiff,\n",
    ")\n",
    "from careamics.lvae_training.eval_utils import (\n",
    "    Calibration,\n",
    "    get_calibrated_factor_for_stdev,\n",
    "    get_dset_predictions,\n",
    "    get_eval_output_dir,\n",
    "    plot_calibration,\n",
    "    plot_error,\n",
    "    show_for_one,\n",
    "    stitch_predictions,\n",
    ")\n",
    "from careamics.models.lvae.noise_models import noise_model_factory\n",
    "from careamics.utils.metrics import (\n",
    "    # avg_psnr,\n",
    "    # avg_range_inv_psnr,\n",
    "    # avg_ssim,\n",
    "    scale_invariant_psnr,\n",
    "    # multiscale_ssim\n",
    ")\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seeds():\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/group/jug/federico/careamics_training/data/BioSR'\n",
    "OUT_ROOT = '/group/jug/federico/careamics_training/refac_v2/'\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = os.path.join(OUT_ROOT, '2409/musplit_with_LC/1/')\n",
    "assert os.path.exists(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Set Evaluation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set eval parameters\n",
    "mmse_count: int = 10\n",
    "\"\"\"The number of predictions to average for MMSE evaluation.\"\"\"\n",
    "image_size_for_grid_centers: int = 32\n",
    "\"\"\"The size of the portion of image we retain from inner padding/tiling.\"\"\"\n",
    "eval_patch_size: Optional[int] = 64\n",
    "\"\"\"The actual patch size. If not specified data.image_size.\"\"\"\n",
    "psnr_type: Literal['simple', 'range_invariant'] = 'range_invariant'\n",
    "\"\"\"The type of PSNR to compute.\"\"\"\n",
    "which_ckpt: Literal['best', 'last'] = 'best'\n",
    "\"\"\"Which checkpoint to use for evaluation.\"\"\"\n",
    "enable_calibration: bool = False\n",
    "\"\"\"Whether to enable calibration.\"\"\"\n",
    "eval_datasplit_type = DataSplitType.Test\n",
    "\"\"\"The data split used for evaluation.\"\"\"\n",
    "batch_size: int = 32\n",
    "\"\"\"Batch size for data loader.\"\"\"\n",
    "num_workers = 4\n",
    "\"\"\"Number of workers for data loader.\"\"\"\n",
    "use_deterministic_grid = None\n",
    "\"\"\"Whether to use a deterministic grid for evaluation (i.e., get a random patch).\"\"\"\n",
    "data_t_list = None\n",
    "\"\"\"List of indexes of the data to be used (e.g., for debugging or picking a particular image).\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Load config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_checkpoint(\n",
    "    ckpt_dir: str, mode: Literal['best', 'last'] = 'best'\n",
    ") -> str:\n",
    "    output = []\n",
    "    for fpath in glob.glob(ckpt_dir + \"/*.ckpt\"):\n",
    "        fname = os.path.basename(fpath)\n",
    "        if mode == 'best':\n",
    "            if fname.startswith('best'):\n",
    "                output.append(fpath)\n",
    "        elif mode == 'last':\n",
    "            if fname.startswith('last'):\n",
    "                output.append(fpath)\n",
    "    assert len(output) == 1, '\\n'.join(output)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path: str):\n",
    "    # Get the file extension\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "\n",
    "    # Check the extension and load the file accordingly\n",
    "    if ext == '.pkl':\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif ext == '.json':\n",
    "        with open(file_path) as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}. Only .pkl and .json are supported.\")\n",
    "\n",
    "def load_config(config_fpath: str, config_type: Literal['algorithm', 'training', 'data']) -> dict:\n",
    "    for fname in glob.glob(os.path.join(config_fpath, '*config.*')):\n",
    "        fname = os.path.basename(fname)\n",
    "        if fname.startswith(config_type):\n",
    "            return load_file(os.path.join(config_fpath, fname))\n",
    "    raise ValueError(f\"Config file not found in {config_fpath}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(ckpt_dir):\n",
    "    algo_config = load_config(ckpt_dir, \"algorithm\")\n",
    "    training_config = load_config(ckpt_dir, \"training\")\n",
    "    data_config = load_config(ckpt_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    config: dict,\n",
    "    datadir: str,\n",
    "    eval_datasplit_type = DataSplitType.Val,\n",
    "    skip_train_dataset = False,\n",
    "    kwargs_dict = None,\n",
    ") -> tuple[Dataset, Dataset, tuple[float, float]]:\n",
    "    if kwargs_dict is None:\n",
    "        kwargs_dict = {}\n",
    "\n",
    "    datapath = datadir\n",
    "\n",
    "    # Hard-coded parameters (used to be in the config file)\n",
    "    normalized_input = True\n",
    "    use_one_mu_std = True\n",
    "    train_aug_rotate = False\n",
    "    enable_random_cropping = True\n",
    "    lowres_supervision = False\n",
    "\n",
    "    # 1) Data loader for Lateral Contextualization\n",
    "    if config.multiscale_lowres_count > 1:\n",
    "        # Get padding attributes\n",
    "        if \"padding_kwargs\" not in kwargs_dict:\n",
    "            padding_kwargs = {\"mode\": \"reflect\"}\n",
    "        else:\n",
    "            padding_kwargs = kwargs_dict.pop(\"padding_kwargs\")\n",
    "\n",
    "        train_data = (\n",
    "            None\n",
    "            if skip_train_dataset\n",
    "            else LCMultiChDloader(\n",
    "                config,\n",
    "                datapath,\n",
    "                datasplit_type=DataSplitType.Train,\n",
    "                val_fraction=0.1,\n",
    "                test_fraction=0.1,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=train_aug_rotate,\n",
    "                enable_random_cropping=enable_random_cropping,\n",
    "                num_scales=config.multiscale_lowres_count,\n",
    "                lowres_supervision=lowres_supervision,\n",
    "                padding_kwargs=padding_kwargs,\n",
    "                **kwargs_dict,\n",
    "                allow_generation=True,\n",
    "            )\n",
    "        )\n",
    "        max_val = train_data.get_max_val()\n",
    "\n",
    "        val_data = LCMultiChDloader(\n",
    "            config,\n",
    "            datapath,\n",
    "            datasplit_type=eval_datasplit_type,\n",
    "            val_fraction=0.1,\n",
    "            test_fraction=0.1,\n",
    "            normalized_input=normalized_input,\n",
    "            use_one_mu_std=use_one_mu_std,\n",
    "            enable_rotation_aug=False,  # No rotation aug on validation\n",
    "            enable_random_cropping=False,\n",
    "            # No random cropping on validation. Validation is evaluated on determistic grids\n",
    "            num_scales=config.multiscale_lowres_count,\n",
    "            lowres_supervision=lowres_supervision,\n",
    "            padding_kwargs=padding_kwargs,\n",
    "            allow_generation=False,\n",
    "            **kwargs_dict,\n",
    "            max_val=max_val,\n",
    "        )\n",
    "    # 2) Vanilla data loader\n",
    "    else:\n",
    "        train_data_kwargs = {\"allow_generation\": True, **kwargs_dict}\n",
    "        val_data_kwargs = {\"allow_generation\": False, **kwargs_dict}\n",
    "\n",
    "        train_data_kwargs[\"enable_random_cropping\"] = enable_random_cropping\n",
    "        val_data_kwargs[\"enable_random_cropping\"] = False\n",
    "\n",
    "        train_data = (\n",
    "            None\n",
    "            if skip_train_dataset\n",
    "            else MultiChDloader(\n",
    "                data_config=config,\n",
    "                fpath=datapath,\n",
    "                datasplit_type=DataSplitType.Train,\n",
    "                val_fraction=0.1,\n",
    "                test_fraction=0.1,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=train_aug_rotate,\n",
    "                **train_data_kwargs,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        max_val = train_data.get_max_val()\n",
    "        val_data = MultiChDloader(\n",
    "            data_config=config,\n",
    "            fpath=datapath,\n",
    "            datasplit_type=eval_datasplit_type,\n",
    "            val_fraction=0.1,\n",
    "            test_fraction=0.1,\n",
    "            normalized_input=normalized_input,\n",
    "            use_one_mu_std=use_one_mu_std,\n",
    "            enable_rotation_aug=False,  # No rotation aug on validation\n",
    "            max_val=max_val,\n",
    "            **val_data_kwargs,\n",
    "        )\n",
    "\n",
    "    mean_val, std_val = train_data.compute_mean_std()\n",
    "    train_data.set_mean_std(mean_val, std_val)\n",
    "    val_data.set_mean_std(mean_val, std_val)\n",
    "    data_stats = train_data.get_mean_std()\n",
    "\n",
    "    # NOTE: \"input\" mean & std are computed over the entire dataset and repeated for each channel.\n",
    "    # On the contrary, \"target\" mean & std are computed separately for each channel.\n",
    "\n",
    "    return train_data, val_data, data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some params\n",
    "# TODO: check if this is right\n",
    "if eval_patch_size is not None:\n",
    "    training_image_size = data_config[\"image_size\"]\n",
    "    data_config[\"image_size\"] = eval_patch_size\n",
    "\n",
    "if image_size_for_grid_centers is not None:\n",
    "    training_grid_size = data_config.get(\"grid_size\", \"grid_size not present\")\n",
    "    data_config[\"grid_size\"] = image_size_for_grid_centers\n",
    "\n",
    "padding_kwargs = {\"mode\": \"reflect\"}\n",
    "dset_kwargs = {\n",
    "    \"overlapping_padding_kwargs\": padding_kwargs,\n",
    "    \"grid_alignment\": GridAlignement.Center\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset, val_dset, data_stats = create_dataset(\n",
    "    config=ml_collections.ConfigDict(data_config),\n",
    "    datadir=DATA_DIR,\n",
    "    eval_datasplit_type=eval_datasplit_type,\n",
    "    kwargs_dict=dset_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = data_stats[0][\"target\"]\n",
    "data_std = data_stats[1][\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Noise-free dataset (High-SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset without poisson noise as ground truth\n",
    "tmp_config = deepcopy(data_config)\n",
    "if 'poisson_noise_factor' in tmp_config:\n",
    "    tmp_config[\"poisson_noise_factor\"] = -1\n",
    "if 'enable_gaussian_noise' in tmp_config:\n",
    "    tmp_config[\"enable_gaussian_noise\"] = False\n",
    "\n",
    "_, highsnr_val_dset, _ = create_dataset(\n",
    "    config=ml_collections.ConfigDict(tmp_config),\n",
    "    datadir=DATA_DIR,\n",
    "    eval_datasplit_type=eval_datasplit_type,\n",
    "    kwargs_dict=dset_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Note: noise model and the associated likelihood are not saved in the config, hence we need to reinitialize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load noise model (for denoiSplit)\n",
    "if algo_config[\"algorithm\"] == \"denoisplit\":\n",
    "    nm_config = MultiChannelNMConfig(**algo_config[\"noise_model\"])\n",
    "    noise_model = noise_model_factory(nm_config)\n",
    "\n",
    "    # Add to noise model likelihood config\n",
    "    nm_likelihood_config = NMLikelihoodConfig(\n",
    "        noise_model=noise_model,\n",
    "        data_mean=data_mean,\n",
    "        data_std=data_std,\n",
    "    )\n",
    "\n",
    "    # Add to algo config\n",
    "    algo_config[\"noise_model_likelihood_model\"] = nm_likelihood_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_model = VAEModule(algorithm_config=algo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(ckpt_dir):\n",
    "    ckpt_fpath = get_model_checkpoint(ckpt_dir, mode=which_ckpt)\n",
    "else:\n",
    "    assert os.path.isfile(ckpt_dir)\n",
    "    ckpt_fpath = ckpt_dir\n",
    "\n",
    "print(f\"Loading checkpoint from: '{ckpt_fpath}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_fpath)\n",
    "\n",
    "light_model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "light_model.eval()\n",
    "light_model.cuda()\n",
    "\n",
    "print('Loading weights from epoch', checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in light_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Model has {count_parameters(light_model)/1000_000:.3f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_config[\"multiscale_lowres_count\"] is not None and eval_patch_size is not None:\n",
    "    light_model.model.reset_for_different_output_size(eval_patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### From here on we perform evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Visualize Data: noisy & ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print input (first row) and target (second row) of the val_dset\n",
    "idx = np.random.randint(len(val_dset))\n",
    "inp_tmp, tar_tmp, *_ = val_dset[idx]\n",
    "gt_inp_tmp, gt_tar_tmp, *_ = highsnr_val_dset[idx]\n",
    "\n",
    "# Noisy\n",
    "ncols = len(tar_tmp)\n",
    "nrows = 2\n",
    "_, ax = plt.subplots(figsize=(4*ncols,4*nrows), ncols=ncols, nrows=nrows)\n",
    "plt.suptitle(\"Noisy patches\")\n",
    "for i in range(min(ncols, len(inp_tmp))):\n",
    "    ax[0,i].imshow(inp_tmp[i])\n",
    "\n",
    "for channel_id in range(ncols):\n",
    "    ax[1,channel_id].imshow(tar_tmp[channel_id])\n",
    "\n",
    "# Ground truth\n",
    "ncols = len(gt_tar_tmp)\n",
    "_, ax = plt.subplots(figsize=(4*ncols,4*nrows), ncols=ncols, nrows=nrows)\n",
    "plt.suptitle(\"Ground Truth patches\")\n",
    "for i in range(min(ncols, len(gt_inp_tmp))):\n",
    "    ax[0,i].imshow(gt_inp_tmp[i])\n",
    "\n",
    "for channel_id in range(ncols):\n",
    "    ax[1,channel_id].imshow(gt_tar_tmp[channel_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "##### For debugging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_input_frame(\n",
    "    idx: int, dset: Union[MultiChDloader, LCMultiChDloader]\n",
    ") -> tuple[torch.Tensor, int, int]:\n",
    "    \"\"\"Get the full input frame for a given index.\"\"\"\n",
    "    img_tuples, noise_tuples = dset._load_img(idx)\n",
    "    if len(noise_tuples) > 0:\n",
    "        factor = np.sqrt(2) if dset._input_is_sum else 1.0\n",
    "        img_tuples = [x + noise_tuples[0] * factor for x in img_tuples]\n",
    "\n",
    "    inp = 0\n",
    "    for nch in img_tuples:\n",
    "        inp += nch/len(img_tuples)\n",
    "    h_start, w_start = dset._get_deterministic_hw(idx)\n",
    "    return inp, h_start, w_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the data (e.g., for debugging)\n",
    "if DEBUG and data_t_list is not None:\n",
    "    val_dset.reduce_data(t_list=data_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    index = np.random.randint(len(val_dset))\n",
    "    inp, tar = val_dset[index]\n",
    "    frame, h_start, w_start = get_full_input_frame(index, val_dset)\n",
    "    print(inp.shape, h_start, w_start)\n",
    "    plt.imshow(inp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions against a baseline for specific indexes\n",
    "if DEBUG:\n",
    "    def get_hwt_start(idx):\n",
    "        h,w,t = val_dset.idx_manager.hwt_from_idx(idx, grid_size=64)\n",
    "        print(h,w,t)\n",
    "        pad = val_dset.per_side_overlap_pixelcount()\n",
    "        h =  h - pad\n",
    "        w = w - pad\n",
    "        return h,w,t\n",
    "\n",
    "    def get_crop_from_fulldset_prediction(full_dset_pred, idx, patch_size=256):\n",
    "        h,w,t = get_hwt_start(idx)\n",
    "        return np.swapaxes(full_dset_pred[t,h:h+patch_size,w:w+patch_size].astype(np.float32)[None], 0, 3)[...,0]\n",
    "\n",
    "    if save_comparative_plots: # this is false...\n",
    "        assert eval_datasplit_type == DataSplitType.Test\n",
    "        # CCP vs Microtubules: 925, 659, 502\n",
    "        # hdn_usplitdata = load_tiff('/group/jug/ashesh/data/paper_stats/Test_PNone_G16_M3_Sk0/pred_disentangle_2402_D23-M3-S0-L0_67.tif')\n",
    "        hdn_usplitdata = load_tiff('/group/jug/ashesh/data/paper_stats/Test_PNone_G32_M5_Sk0/pred_disentangle_2403_D23-M3-S0-L0_29.tif')\n",
    "\n",
    "        # ER vs Microtubule 853, 859, 332\n",
    "        # hdn_usplitdata = load_tiff('/group/jug/ashesh/data/paper_stats/Test_PNone_G16_M3_Sk0/pred_disentangle_2402_D23-M3-S0-L0_60.tif')\n",
    "\n",
    "        #  ER vs CCP 327, 479, 637, 568\n",
    "        # hdn_usplitdata = load_tiff('/group/jug/ashesh/data/paper_stats/Test_PNone_G16_M3_Sk0/pred_disentangle_2402_D23-M3-S0-L0_59.tif')\n",
    "\n",
    "        #  F-actin vs ER 797\n",
    "        # hdn_usplitdata = load_tiff('/group/jug/ashesh/data/paper_stats/Test_PNone_G32_M10_Sk0/pred_disentangle_2403_D23-M3-S0-L0_15.tif')\n",
    "\n",
    "        idx = 10 #np.random.randint(len(val_dset))\n",
    "        patch_size = 500\n",
    "        mmse_count = 50\n",
    "        print(idx)\n",
    "        show_for_one(\n",
    "            idx, val_dset,\n",
    "            highsnr_val_dset,\n",
    "            model,\n",
    "            None,\n",
    "            mmse_count=mmse_count,\n",
    "            patch_size=patch_size,\n",
    "            baseline_preds=[\n",
    "                get_crop_from_fulldset_prediction(hdn_usplitdata, idx).astype(np.float32),\n",
    "            ],\n",
    "            num_samples=0\n",
    "        )\n",
    "\n",
    "        plotsdir = get_plots_output_dir(\n",
    "            ckpt_dir,\n",
    "            patch_size,\n",
    "            mmse_count=mmse_count\n",
    "        )\n",
    "\n",
    "        model_id = ckpt_dir.strip('/').split('/')[-1]\n",
    "        fname = f'patch_comparison_{idx}_{model_id}.png'\n",
    "        fpath = os.path.join(plotsdir, fname)\n",
    "        plt.savefig(fpath, dpi=200, bbox_inches='tight')\n",
    "        print(f'Saved to {fpath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Compute predictions and related metrics (PSNR) for the entire validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: here, patch-wise PSNR is used, hence results are not trustworthy\n",
    "pred_tiled, pred_std_tiled, logvar_tiled, rec_loss, patch_psnr_tuple,  = get_dset_predictions(\n",
    "  model=light_model,\n",
    "  dset=val_dset,\n",
    "  batch_size=batch_size,\n",
    "  num_workers=num_workers,\n",
    "  mmse_count=mmse_count,\n",
    "  loss_type=algo_config[\"loss\"],\n",
    ")\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple], 2)\n",
    "print(f\"Patch wise PSNR, as computed during training {tmp}, avg: {np.mean(tmp)}\")\n",
    "print(f'Number of predicted tiles: {pred_tiled.shape[0]}, channels: {pred_tiled.shape[1]}, shape: {pred_tiled.shape[2:]}')\n",
    "print(f'Reconstruction loss distrib: {np.quantile(rec_loss, [0,0.01,0.5, 0.9,0.99,0.999,1]).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tiles in which the logvar is very low\n",
    "if DEBUG:\n",
    "    idx_list = np.where(logvar_tiled.squeeze() < -6)[0]\n",
    "    if len(idx_list) > 0:\n",
    "        plt.imshow(val_dset[idx_list[0]][1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Get full image predictions by stitching the predicted tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_tiled.shape[-1] != val_dset.get_img_sz():\n",
    "    pad = (val_dset.get_img_sz() - pred_tiled.shape[-1] )//2\n",
    "    pred_tiled = np.pad(pred_tiled, ((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "\n",
    "# Stitch tiled predictions\n",
    "pred = stitch_predictions(\n",
    "    pred_tiled,\n",
    "    val_dset,\n",
    "    smoothening_pixelcount=0\n",
    ")\n",
    "\n",
    "# # Stitch predicted tiled logvar\n",
    "# if len(np.unique(logvar_tiled)) == 1:\n",
    "#     logvar = None\n",
    "# else:\n",
    "#     logvar = stitch_predictions(logvar_tiled, val_dset, smoothening_pixelcount=0) # TODO: there's a bug here\n",
    "\n",
    "# Stitch the std of the predictions (i.e., std computed on the mmse_count predictions)\n",
    "pred_std = stitch_predictions(pred_std_tiled, val_dset, smoothening_pixelcount=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    if 'target_idx_list' in data_config and data_config[\"target_idx_list\"] is not None:\n",
    "        pred = pred[...,:len(data_config[\"target_idx_list\"])]\n",
    "        pred_std = pred_std[...,:len(data_config[\"target_idx_list\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Ignore (and remove) the pixels which are present in the last few rows and columns (since not multiples of patch_size)\n",
    "1. They don't come in the batches. So, in prediction, they are simply zeros. So they are being are ignored right now. \n",
    "2. For the border pixels which are on the top and the left, overlapping yields worse performance. This is becuase, there is nothing to overlap on one side. So, they are essentially zero padded. This makes the performance worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ignored_pixels():\n",
    "    \"\"\"Get the number of ignored pixels in the predictions.\n",
    "    \n",
    "    Given the current predictions `pred`, analyze the first image std\n",
    "    to find the number of pixels that are ignored in prediction.\n",
    "    \"\"\"\n",
    "    ignored_pixels = 1\n",
    "    while(pred[0, -ignored_pixels:, -ignored_pixels:,].std() == 0):\n",
    "        ignored_pixels+=1\n",
    "    ignored_pixels-=1\n",
    "    print(f'In {pred.shape}, last {ignored_pixels} many rows and columns are all zero.')\n",
    "    return ignored_pixels\n",
    "\n",
    "actual_ignored_pixels = get_ignored_pixels()\n",
    "print(f'Actual ignored pixels: {actual_ignored_pixels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_config[\"data_type\"] in [\n",
    "    DataType.OptiMEM100_014,\n",
    "    DataType.SemiSupBloodVesselsEMBL,\n",
    "    DataType.Pavia2VanillaSplitting,\n",
    "    DataType.ExpansionMicroscopyMitoTub,\n",
    "    DataType.ShroffMitoEr,\n",
    "    DataType.HTIba1Ki67\n",
    "]:\n",
    "    ignored_last_pixels = 32\n",
    "elif data_config[\"data_type\"] == DataType.BioSR_MRC:\n",
    "    ignored_last_pixels = 44\n",
    "elif data_config[\"data_type\"] == DataType.NicolaData:\n",
    "    ignored_last_pixels = 8\n",
    "else:\n",
    "    ignored_last_pixels = 0\n",
    "\n",
    "ignore_first_pixels = 0\n",
    "# assert actual_ignored_pixels <= ignored_last_pixels, f'Set ignored_last_pixels={actual_ignored_pixels}' # TODO: check this once stitching is fixed\n",
    "print(ignored_last_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = val_dset._data\n",
    "\"\"\"Data used to do evaluation againts. Shape is (N, H, W, C).\n",
    "\n",
    "NOTE: this is the original data (`dset._data`), hence not normalized!\n",
    "\"\"\"\n",
    "\n",
    "if DEBUG:\n",
    "    if 'target_idx_list' in data_config and data_config.target_idx_list is not None:\n",
    "        tar = tar[..., data_config.target_idx_list]\n",
    "\n",
    "def ignore_pixels(\n",
    "    arr: Union[np.ndarray, torch.Tensor],\n",
    "    patch_size: int\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    \"\"\"Remove pixels that are ignored in the predictions.\"\"\"\n",
    "    if arr.shape[2] % patch_size:\n",
    "        if ignore_first_pixels:\n",
    "            arr = arr[:,ignore_first_pixels:,ignore_first_pixels:]\n",
    "        if ignored_last_pixels:\n",
    "            arr = arr[:,:-ignored_last_pixels,:-ignored_last_pixels]\n",
    "    return arr\n",
    "\n",
    "pred = ignore_pixels(pred, val_dset.get_img_sz())\n",
    "tar = ignore_pixels(tar, val_dset.get_img_sz())\n",
    "if pred_std is not None:\n",
    "    pred_std = ignore_pixels(pred_std, val_dset.get_img_sz())\n",
    "\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Perform Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: not needed, target is already normalized\n",
    "sep_mean = np.transpose(data_mean, axes=(0, 2, 3, 1))\n",
    "sep_std = np.transpose(data_std, axes=(0, 2, 3, 1))\n",
    "\n",
    "tar_normalized = (tar - sep_mean)/ sep_std\n",
    "\n",
    "# Check if normalization is correct (i.e., not already applied on tar)\n",
    "print(f\"Channelwise means: tar -> {tar.mean(axis=(0,1,2))}, normalized -> {tar_normalized.mean(axis=(0,1,2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Plot RMV vs. RMSE without Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Recall the `pred_std` here is the pixel-wise std of the mmse_count many predictions\n",
    "if enable_calibration:\n",
    "    calib = Calibration(\n",
    "        num_bins=30,\n",
    "        mode='pixelwise'\n",
    "    )\n",
    "    native_stats = calib.compute_stats(\n",
    "        pred=pred,\n",
    "        pred_logvar=pred_std,\n",
    "        target=tar_normalized\n",
    "    )\n",
    "    count = np.array(native_stats[0]['bin_count'])\n",
    "    count = count / count.sum()\n",
    "    # print(count.cumsum()[:-1])\n",
    "    plt.plot(native_stats[0]['rmv'][1:-1], native_stats[0]['rmse'][1:-1], 'o')\n",
    "    plt.title(\"RMV vs. RMSE plot - Not Calibrated\")\n",
    "    plt.xlabel('RMV'), plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Observe that the plot is far from resembling y = x!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_calibration_fnames(ckpt_dir):\n",
    "#     tokens = ckpt_dir.strip('/').split('/')\n",
    "#     modelid = int(tokens[-1])\n",
    "#     model_specs = tokens[-2].replace('-','')\n",
    "#     monthyear = tokens[-3]\n",
    "#     fname_factor = f'calibration_factor_{monthyear}_{model_specs}_{modelid}.npy'\n",
    "#     fname_stats = f'calibration_stats_{monthyear}_{model_specs}_{modelid}.pkl.npy'\n",
    "#     return {'stats': fname_stats, 'factor': fname_factor}\n",
    "\n",
    "# def get_calibration_factor_fname(ckpt_dir):\n",
    "#     return get_calibration_fnames(ckpt_dir)['factor']\n",
    "\n",
    "# def get_calibration_stats_fname(ckpt_dir):\n",
    "#     return get_calibration_fnames(ckpt_dir)['stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_calibration:\n",
    "    inp, _ = val_dset[0]\n",
    "    out_dir = get_eval_output_dir(ckpt_dir, inp.shape[1], mmse_count=mmse_count)\n",
    "    fname = \"calibration_factor.npy\"\n",
    "    factor_fpath = os.path.join(out_dir, fname)\n",
    "\n",
    "    # Compute calibration factors\n",
    "    if eval_datasplit_type == DataSplitType.Val:\n",
    "        # Compute calibration factors for the channels\n",
    "        calib_factors = []\n",
    "        for i in range(pred.shape[-1]):\n",
    "            calib_factors.append(\n",
    "                get_calibrated_factor_for_stdev(\n",
    "                    pred=pred[..., i],\n",
    "                    pred_logvar=np.log(pred_std[..., i] ** 2),\n",
    "                    target=tar_normalized[..., i],\n",
    "                    batch_size=8,\n",
    "                    lr=0.1\n",
    "                )\n",
    "            )\n",
    "        print(f\"Calibration factors: {[calib_factor for calib_factor in calib_factors]}\")\n",
    "        calib_factor = np.array(calib_factors).reshape(1, 1, 1, 2)\n",
    "        np.save(factor_fpath, calib_factor)\n",
    "        print(f'Saved calibration factor fitted on validation set to {factor_fpath}')\n",
    "\n",
    "    # Use pre-computed calibration factor\n",
    "    elif eval_datasplit_type == DataSplitType.Test:\n",
    "        print('Loading the calibration factor from the file', factor_fpath)\n",
    "        calib_factor = np.load(factor_fpath)\n",
    "\n",
    "    # Given the calibration factor, plot RMV vs. RMSE\n",
    "    calib = Calibration(num_bins=30, mode='pixelwise')\n",
    "    pred_logvar = 2* np.log(pred_std * calib_factor)\n",
    "    stats = calib.compute_stats(\n",
    "        pred,\n",
    "        pred_logvar,\n",
    "        tar_normalized\n",
    "    )\n",
    "    _,ax = plt.subplots(figsize=(5,5))\n",
    "    plt.title(\"RMV vs. RMSE plot - Calibrated\")\n",
    "    plot_calibration(ax, stats)\n",
    "\n",
    "    if eval_datasplit_type == DataSplitType.Test:\n",
    "        stats_fpath = os.path.join(out_dir, \"calibration_stats.pkl.npy\")\n",
    "        np.save(stats_fpath, stats)\n",
    "        print('Saved stats of Test set to ', stats_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "A fancier Calibration Plot with multiple calibration factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_index(bin_count, quantile):\n",
    "    cumsum = np.cumsum(bin_count)\n",
    "    normalized_cumsum = cumsum / cumsum[-1]\n",
    "    for i in range(1, len(normalized_cumsum)):\n",
    "        if normalized_cumsum[-i] < quantile:\n",
    "            return i - 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_first_index(bin_count, quantile):\n",
    "    cumsum = np.cumsum(bin_count)\n",
    "    normalized_cumsum = cumsum / cumsum[-1]\n",
    "    for i in range(len(normalized_cumsum)):\n",
    "        if normalized_cumsum[i] > quantile:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: left for future refactoring\n",
    "if enable_calibration:\n",
    "    try:\n",
    "        calib_factors = [\n",
    "            np.load(os.path.join('/path/to/calibration/factors/dir/', fpath), allow_pickle=True)\n",
    "            for fpath in [\n",
    "                'calibration_stats_1.pkl.npy',\n",
    "                'calibration_stats_2.pkl.npy',\n",
    "                'calibration_stats_3.pkl.npy',\n",
    "            ]\n",
    "        ]\n",
    "        labels = ['w=0.5', 'w=0.9', 'w=1']\n",
    "    except FileNotFoundError:\n",
    "        print('Calibration factors not found. Skipping the plot.')\n",
    "        calib_factors = []\n",
    "\n",
    "    if len(calib_factors) > 0:\n",
    "        _,ax = plt.subplots(figsize=(5,2.5))\n",
    "        for i, calibration_stats in enumerate(calib_factors):\n",
    "            first_idx = get_first_index(calibration_stats[()][0]['bin_count'], 0.0001)\n",
    "            last_idx = get_last_index(calibration_stats[()][0]['bin_count'], 0.9999)\n",
    "            ax.plot(\n",
    "                calibration_stats[()][0]['rmv'][first_idx:-last_idx],\n",
    "                calibration_stats[()][0]['rmse'][first_idx:-last_idx],\n",
    "                '-+',\n",
    "                label=labels[i]\n",
    "            )\n",
    "\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "        ax.plot(np.arange(0,1.5, 0.01), np.arange(0,1.5, 0.01), 'k--')\n",
    "        ax.set_facecolor('xkcd:light grey')\n",
    "        plt.legend(loc='lower right')\n",
    "        # plt.xlim(0,3)\n",
    "        # plt.ylim(0,1.25)\n",
    "        plt.xlabel('RMV')\n",
    "        plt.ylabel('RMSE')\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "        plotsdir = get_plots_output_dir(ckpt_dir, 0, mmse_count=0)\n",
    "        model_id = ckpt_dir.strip('/').split('/')[-1]\n",
    "        fname = f'calibration_plot_{model_id}.png'\n",
    "        fpath = os.path.join(plotsdir, fname)\n",
    "        # plt.savefig(fpath, dpi=200, bbox_inches='tight')\n",
    "        print(f'Saved to {fpath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Visually compare Targets and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One random target vs predicted image (patch of shape [sz x sz])\n",
    "ncols = tar.shape[-1]\n",
    "_,ax = plt.subplots(figsize=(ncols*5, 2*5), nrows=2, ncols=ncols)\n",
    "img_idx = 0\n",
    "sz = 800\n",
    "hs = np.random.randint(tar.shape[1] - sz)\n",
    "ws = np.random.randint(tar.shape[2] - sz)\n",
    "for i in range(ncols):\n",
    "    ax[i,0].set_title(f'Target Channel {i+1}')\n",
    "    ax[i,0].imshow(tar[0, hs:hs+sz, ws:ws+sz, i])\n",
    "    ax[i,1].set_title(f'Predicted Channel {i+1}')\n",
    "    ax[i,1].imshow(pred[0, hs:hs+sz, ws:ws+sz, i])\n",
    "\n",
    "# plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "# clean_ax(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = pred.shape[-1]\n",
    "img_sz = 3\n",
    "_,ax = plt.subplots(figsize=(4*img_sz,nrows*img_sz), ncols=4, nrows=nrows)\n",
    "idx = np.random.randint(len(pred))\n",
    "print(idx)\n",
    "for ch_id in range(nrows):\n",
    "    ax[ch_id,0].set_title(f'Target Channel {ch_id+1}')\n",
    "    ax[ch_id,0].imshow(tar_normalized[idx,..., ch_id], cmap='magma')\n",
    "    ax[ch_id,1].set_title(f'Predicted Channel {ch_id+1}')\n",
    "    ax[ch_id,1].imshow(pred[idx,:,:,ch_id], cmap='magma')\n",
    "    plot_error(\n",
    "        tar_normalized[idx,...,ch_id],\n",
    "        pred[idx,:,:,ch_id],\n",
    "        cmap = matplotlib.cm.coolwarm,\n",
    "        ax = ax[ch_id,2],\n",
    "        max_val = None\n",
    "    )\n",
    "\n",
    "    cropsz = 256\n",
    "    h_s = np.random.randint(0, tar_normalized.shape[1] - cropsz)\n",
    "    h_e = h_s + cropsz\n",
    "    w_s = np.random.randint(0, tar_normalized.shape[2] - cropsz)\n",
    "    w_e = w_s + cropsz\n",
    "\n",
    "    plot_error(\n",
    "        tar_normalized[idx,h_s:h_e,w_s:w_e, ch_id],\n",
    "        pred[idx,h_s:h_e,w_s:w_e,ch_id],\n",
    "        cmap = matplotlib.cm.coolwarm,\n",
    "        ax = ax[ch_id,3],\n",
    "        max_val = None\n",
    "    )\n",
    "\n",
    "    # Add rectangle to the region\n",
    "    rect = patches.Rectangle((w_s, h_s), w_e-w_s, h_e-h_s, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax[ch_id,2].add_patch(rect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "#### Compute metrics between predicted data and high-SNR (ground truth) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_unnorm = []\n",
    "for i in range(pred.shape[-1]):\n",
    "    if sep_std.shape[-1] == 1:\n",
    "        temp_pred_unnorm = pred[...,i] * sep_std[...,0] + sep_mean[...,0]\n",
    "    else:\n",
    "        temp_pred_unnorm = pred[...,i] * sep_std[...,i] + sep_mean[...,i]\n",
    "    pred_unnorm.append(temp_pred_unnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get & process high-SNR data from previously loaded dataset\n",
    "highres_data = highsnr_val_dset._data\n",
    "if highres_data is not None:\n",
    "    highres_data = ignore_pixels(highres_data, highsnr_val_dset.get_img_sz()).copy()\n",
    "    if data_t_list is not None:\n",
    "        highres_data = highres_data[data_t_list].copy()\n",
    "\n",
    "    if \"target_idx_list\" in data_config and data_config[\"target_idx_list\"] is not None:\n",
    "        highres_data = highres_data[..., data_config[\"target_idx_list\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_range_inv_psnr(\n",
    "    pred: np.ndarray,\n",
    "    target: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Compute the average range-invariant PSNR.\"\"\"\n",
    "    psnr_arr = []\n",
    "    for i in range(pred.shape[0]):\n",
    "        psnr_arr.append(scale_invariant_psnr(pred[i], target[i]))\n",
    "    return np.mean(psnr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if highres_data is not None:\n",
    "    print(f'{DataSplitType.name(eval_datasplit_type)}_P{eval_patch_size}_G{image_size_for_grid_centers}_M{mmse_count}_Sk{ignored_last_pixels}')\n",
    "    psnr_list = [avg_range_inv_psnr(highres_data[...,k], pred_unnorm[k]) for k in range(len(pred_unnorm))]\n",
    "    highres_norm = (highres_data - sep_mean) / sep_std\n",
    "    # care_ssim_list = multiscale_ssim(highres_norm, pred)\n",
    "    print(f\"PSNR on Highres: {' '.join([str(x) for x in psnr_list])}, avg: {np.mean(psnr_list)}\")\n",
    "    # print(f\"CARE-SSIM on Highres: {' '.join([str(np.round(x,3)) for x in care_ssim_list])}, avg: {np.mean(care_ssim_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_arr = []\n",
    "psnr_arr = []\n",
    "rinv_psnr_arr = []\n",
    "ssim_arr = []\n",
    "for ch_id in range(pred.shape[-1]):\n",
    "    rmse =np.sqrt(((pred[...,ch_id] - tar_normalized[...,ch_id])**2).reshape(len(pred),-1).mean(axis=1))\n",
    "    rmse_arr.append(rmse)\n",
    "    psnr = avg_psnr(tar_normalized[...,ch_id].copy(), pred[...,ch_id].copy())\n",
    "    rinv_psnr = avg_range_inv_psnr(tar_normalized[...,ch_id].copy(), pred[...,ch_id].copy())\n",
    "    ssim_mean, ssim_std = avg_ssim(tar[...,ch_id], pred_unnorm[ch_id])\n",
    "    psnr_arr.append(psnr)\n",
    "    rinv_psnr_arr.append(rinv_psnr)\n",
    "    ssim_arr.append((ssim_mean,ssim_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{DataSplitType.name(eval_datasplit_type)}_P{eval_patch_size}_G{image_size_for_grid_centers}_M{mmse_count}_Sk{ignored_last_pixels}')\n",
    "print('Rec Loss: ', np.round(rec_loss.mean(),3) )\n",
    "print('RMSE: ', ' <--> '.join([str(np.mean(x).round(3)) for x in rmse_arr]))\n",
    "print('PSNR: ', ' <--> '.join([str(x) for x in psnr_arr]))\n",
    "print('RangeInvPSNR: ',' <--> '.join([str(x) for x in rinv_psnr_arr]))\n",
    "print('SSIM: ',' <--> '.join([f'{round(x,3)}±{round(y,4)}' for (x,y) in ssim_arr]))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usplit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "e959a19f8af3b4149ff22eb57702a46c14a8caae5a2647a6be0b1f60abdfa4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
